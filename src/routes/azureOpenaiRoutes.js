const express = require('express')
const router = express.Router()
const logger = require('../utils/logger')
const { authenticateApiKey } = require('../middleware/auth')
const azureOpenaiAccountService = require('../services/azureOpenaiAccountService')
const azureOpenaiRelayService = require('../services/azureOpenaiRelayService')
const apiKeyService = require('../services/apiKeyService')
const crypto = require('crypto')

// æ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨ - åŸºäºŽçœŸå®žçš„ Azure OpenAI æ¨¡åž‹
const ALLOWED_MODELS = {
  CHAT_MODELS: [
    'gpt-4',
    'gpt-4-turbo',
    'gpt-4o',
    'gpt-4o-mini',
    'gpt-35-turbo',
    'gpt-35-turbo-16k'
  ],
  CODEX_MODELS: ['codex-mini', 'codex-mini-latest'] // ä¿ç•™ codex-mini æ”¯æŒ
}

const ALL_ALLOWED_MODELS = [...ALLOWED_MODELS.CHAT_MODELS, ...ALLOWED_MODELS.CODEX_MODELS]

// Azure OpenAI ç¨³å®š API ç‰ˆæœ¬
const AZURE_API_VERSION = '2024-02-01'

// åŽŸå­ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Šå™¨
class AtomicUsageReporter {
  constructor() {
    this.reportedUsage = new Set()
    this.pendingReports = new Map()
  }

  async reportOnce(requestId, usageData, apiKeyId, modelToRecord, accountId) {
    if (this.reportedUsage.has(requestId)) {
      logger.debug(`Usage already reported for request: ${requestId}`)
      return false
    }

    // é˜²æ­¢å¹¶å‘é‡å¤æŠ¥å‘Š
    if (this.pendingReports.has(requestId)) {
      return this.pendingReports.get(requestId)
    }

    const reportPromise = this._performReport(
      requestId,
      usageData,
      apiKeyId,
      modelToRecord,
      accountId
    )
    this.pendingReports.set(requestId, reportPromise)

    try {
      const result = await reportPromise
      this.reportedUsage.add(requestId)
      return result
    } finally {
      this.pendingReports.delete(requestId)
      // æ¸…ç†è¿‡æœŸçš„å·²æŠ¥å‘Šè®°å½•
      setTimeout(() => this.reportedUsage.delete(requestId), 60 * 1000) // 1åˆ†é’ŸåŽæ¸…ç†
    }
  }

  async _performReport(requestId, usageData, apiKeyId, modelToRecord, accountId) {
    try {
      const inputTokens = usageData.prompt_tokens || usageData.input_tokens || 0
      const outputTokens = usageData.completion_tokens || usageData.output_tokens || 0
      const cacheCreateTokens =
        usageData.prompt_tokens_details?.cached_tokens ||
        usageData.input_tokens_details?.cache_creation_tokens ||
        0
      const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0

      await apiKeyService.recordUsage(
        apiKeyId,
        inputTokens,
        outputTokens,
        cacheCreateTokens,
        cacheReadTokens,
        modelToRecord,
        accountId
      )

      logger.info(
        `ðŸ“Š Recorded Azure OpenAI usage - Request: ${requestId}, Input: ${inputTokens}, Output: ${outputTokens}, Model: ${modelToRecord}`
      )
      return true
    } catch (error) {
      logger.error(`Failed to record usage for request ${requestId}:`, error)
      throw error
    }
  }

  getStats() {
    return {
      reportedCount: this.reportedUsage.size,
      pendingCount: this.pendingReports.size
    }
  }
}

const usageReporter = new AtomicUsageReporter()

// Azure ç«¯ç‚¹éªŒè¯ (æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä¾›æœªæ¥ä½¿ç”¨)
function _validateAzureEndpoint(endpoint) {
  if (!endpoint) {
    return false
  }
  const azurePattern = /^https:\/\/[\w-]+\.openai\.azure\.com$/
  return azurePattern.test(endpoint)
}

// Azure éƒ¨ç½²éªŒè¯ (æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä¾›æœªæ¥ä½¿ç”¨)
async function _validateAzureDeployment(
  endpoint,
  deploymentName,
  apiKey,
  apiVersion = AZURE_API_VERSION
) {
  try {
    const axios = require('axios')
    const response = await axios.get(
      `${endpoint}/openai/deployments/${deploymentName}?api-version=${apiVersion}`,
      {
        headers: { 'api-key': apiKey },
        timeout: 10000
      }
    )
    return response.status === 200
  } catch (error) {
    logger.warn(`Azure deployment validation failed for ${deploymentName}:`, error.message)
    return false
  }
}

// Azure é”™è¯¯è§„èŒƒåŒ–
function normalizeAzureError(error) {
  const errorData = error.response?.data

  if (errorData?.error?.code === 'DeploymentNotFound') {
    return {
      error: {
        message: 'Model deployment not found',
        type: 'invalid_request_error',
        code: 'deployment_not_found'
      }
    }
  }

  if (errorData?.error?.code === 'RateLimitExceeded') {
    return {
      error: {
        message: 'Rate limit exceeded for Azure OpenAI',
        type: 'rate_limit_error',
        code: 'rate_limit_exceeded'
      }
    }
  }

  if (errorData?.error?.code === 'InvalidAuthentication') {
    return {
      error: {
        message: 'Invalid Azure OpenAI API key',
        type: 'authentication_error',
        code: 'invalid_api_key'
      }
    }
  }

  // é»˜è®¤é”™è¯¯å¤„ç†
  return {
    error: {
      message: errorData?.error?.message || error.message || 'Azure OpenAI request failed',
      type: errorData?.error?.type || 'api_error',
      code: errorData?.error?.code || null
    }
  }
}

// æ¨¡åž‹éªŒè¯å‡½æ•°
function validateAndNormalizeModel(requestedModel, endpoint = 'chat/completions') {
  if (!requestedModel) {
    return endpoint === 'chat/completions' ? 'gpt-4' : 'codex-mini'
  }

  // ç§»é™¤å¯èƒ½çš„å‰ç¼€
  let normalizedModel = requestedModel.replace(/^azure[/:]/, '')

  // å¤„ç†ä¸€äº›å¸¸è§çš„æ¨¡åž‹åˆ«å
  const modelAliases = {
    'gpt-4o-latest': 'gpt-4o',
    'gpt-4-latest': 'gpt-4',
    'gpt-35-turbo-latest': 'gpt-35-turbo',
    'gpt-3.5-turbo': 'gpt-35-turbo',
    'gpt-3.5-turbo-16k': 'gpt-35-turbo-16k'
  }

  normalizedModel = modelAliases[normalizedModel] || normalizedModel

  // éªŒè¯æ¨¡åž‹æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­
  if (!ALL_ALLOWED_MODELS.includes(normalizedModel)) {
    logger.warn(`Invalid model requested: ${requestedModel}, using default`)
    return endpoint === 'chat/completions' ? 'gpt-4' : 'codex-mini'
  }

  return normalizedModel
}

// å®‰å…¨çš„ä¼šè¯å“ˆå¸Œç”Ÿæˆ
function generateSecureSessionHash(sessionId, config) {
  if (!sessionId) {
    return null
  }

  const sessionSalt = config?.security?.sessionSalt || 'default-session-salt'
  return crypto.createHmac('sha256', sessionSalt).update(sessionId).digest('hex')
}

// ä½¿ç”¨ Azure OpenAI æœåŠ¡é€‰æ‹©è´¦æˆ· - å¢žå¼ºå®‰å…¨æ€§
async function getAzureOpenAIAccount(apiKeyData, sessionId = null, requestedModel = null) {
  try {
    const config = require('../../config/config')
    // ç”Ÿæˆå®‰å…¨çš„ä¼šè¯å“ˆå¸Œ
    const sessionHash = generateSecureSessionHash(sessionId, config)

    // é€‰æ‹©å¯ç”¨è´¦æˆ·
    const account = await azureOpenaiAccountService.selectAvailableAccount(
      apiKeyData.id,
      sessionHash,
      requestedModel
    )

    if (!account || !account.apiKey) {
      throw new Error('No available Azure OpenAI account found')
    }

    logger.info(`Selected Azure OpenAI account: ${account.name} (${account.id})`)
    return account
  } catch (error) {
    logger.error('Failed to get Azure OpenAI account:', error)
    throw error
  }
}

// é€šç”¨çš„ Azure OpenAI ç«¯ç‚¹å¤„ç†å™¨
async function handleAzureOpenAIEndpoint(req, res, options = {}) {
  const { endpoint = 'chat/completions', defaultModel = 'gpt-5', defaultStream = false } = options

  try {
    // ä»Žä¸­é—´ä»¶èŽ·å– API Key æ•°æ®
    const apiKeyData = req.apiKey || {}

    // ä»Žè¯·æ±‚å¤´æˆ–è¯·æ±‚ä½“ä¸­æå–ä¼šè¯ ID
    const sessionId =
      req.headers['session_id'] ||
      req.headers['x-session-id'] ||
      req.body?.session_id ||
      req.body?.conversation_id ||
      null

    // éªŒè¯å’Œè§„èŒƒåŒ–æ¨¡åž‹
    const requestedModel = validateAndNormalizeModel(req.body?.model || defaultModel, endpoint)
    const isStream = req.body?.stream === defaultStream ? true : req.body?.stream === true

    // ç”Ÿæˆè¯·æ±‚IDç”¨äºŽåŽ»é‡
    const requestId = `${Date.now()}_${Math.random().toString(36).substr(2, 9)}`

    logger.info(
      `ðŸ“¤ Azure OpenAI ${endpoint} request - Model: ${requestedModel}, Stream: ${isStream}, RequestID: ${requestId}`
    )

    // é€‰æ‹©è´¦æˆ·
    const account = await getAzureOpenAIAccount(apiKeyData, sessionId, requestedModel)

    // å¤„ç†è¯·æ±‚
    const upstreamResponse = await azureOpenaiRelayService.handleAzureOpenAIRequest({
      account,
      requestBody: req.body,
      headers: req.headers,
      isStream,
      endpoint
    })

    // è®¾ç½®å“åº”çŠ¶æ€ç 
    res.status(upstreamResponse.status)

    if (isStream) {
      // æµå¼å“åº”å¤„ç†
      await azureOpenaiRelayService.handleStreamResponse(upstreamResponse, res, {
        onEnd: async ({ usageData, actualModel }) => {
          if (usageData) {
            const modelToRecord = actualModel || requestedModel
            await usageReporter.reportOnce(
              requestId,
              usageData,
              apiKeyData.id,
              modelToRecord,
              account.id
            )
          }
        }
      })
    } else {
      // éžæµå¼å“åº”å¤„ç†
      const { usageData, actualModel } = azureOpenaiRelayService.handleNonStreamResponse(
        upstreamResponse,
        res
      )

      // è®°å½•ä½¿ç”¨ç»Ÿè®¡
      if (usageData) {
        const modelToRecord = actualModel || requestedModel
        await usageReporter.reportOnce(
          requestId,
          usageData,
          apiKeyData.id,
          modelToRecord,
          account.id
        )
      }
    }
  } catch (error) {
    logger.error(`Azure OpenAI ${endpoint} request failed:`, error)
    const status = error.response?.status || 500

    // ä½¿ç”¨ Azure ç‰¹å®šçš„é”™è¯¯è§„èŒƒåŒ–
    const errorResponse = normalizeAzureError(error)

    if (!res.headersSent) {
      res.status(status).json(errorResponse)
    }
  }
}

// Chat Completions ç«¯ç‚¹ (æ¨¡æ‹Ÿ OpenAI API) - ä½¿ç”¨é€šç”¨å¤„ç†å™¨
router.post('/chat/completions', authenticateApiKey, (req, res) =>
  handleAzureOpenAIEndpoint(req, res, {
    endpoint: 'chat/completions',
    defaultModel: 'gpt-5',
    defaultStream: false,
    allowedModels: ALLOWED_MODELS.CHAT_MODELS
  })
)

// Codex Responses ç«¯ç‚¹ (æ”¯æŒ codex-mini æ¨¡åž‹) - ä½¿ç”¨é€šç”¨å¤„ç†å™¨
router.post('/responses', authenticateApiKey, (req, res) =>
  handleAzureOpenAIEndpoint(req, res, {
    endpoint: 'responses',
    defaultModel: 'codex-mini',
    defaultStream: true, // Codexé»˜è®¤ä¸ºæµå¼
    allowedModels: ALLOWED_MODELS.CODEX_MODELS
  })
)

// Models ç«¯ç‚¹ (è¿”å›žæ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨) - åŠ¨æ€ç”Ÿæˆ
router.get('/models', authenticateApiKey, async (req, res) => {
  try {
    // åŠ¨æ€ç”Ÿæˆæ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨
    const currentTime = Math.floor(Date.now() / 1000)

    const modelData = ALL_ALLOWED_MODELS.map((modelId) => ({
      id: modelId,
      object: 'model',
      created: currentTime,
      owned_by: 'azure-openai',
      permission: [],
      root: modelId,
      parent: null
    }))

    const models = {
      object: 'list',
      data: modelData
    }

    res.json(models)
  } catch (error) {
    logger.error('Failed to get Azure OpenAI models:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve models',
        type: 'api_error'
      }
    })
  }
})

// ä½¿ç”¨æƒ…å†µç»Ÿè®¡ç«¯ç‚¹
router.get('/usage', authenticateApiKey, async (req, res) => {
  try {
    const { usage } = req.apiKey

    res.json({
      object: 'usage',
      total_tokens: usage.total.tokens,
      total_requests: usage.total.requests,
      daily_tokens: usage.daily.tokens,
      daily_requests: usage.daily.requests,
      monthly_tokens: usage.monthly.tokens,
      monthly_requests: usage.monthly.requests
    })
  } catch (error) {
    logger.error('Failed to get Azure OpenAI usage stats:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve usage statistics',
        type: 'api_error'
      }
    })
  }
})

// API Key ä¿¡æ¯ç«¯ç‚¹
router.get('/key-info', authenticateApiKey, async (req, res) => {
  try {
    const keyData = req.apiKey
    res.json({
      id: keyData.id,
      name: keyData.name,
      description: keyData.description,
      permissions: keyData.permissions || 'all',
      token_limit: keyData.tokenLimit,
      tokens_used: keyData.usage.total.tokens,
      tokens_remaining:
        keyData.tokenLimit > 0
          ? Math.max(0, keyData.tokenLimit - keyData.usage.total.tokens)
          : null,
      rate_limit: {
        window: keyData.rateLimitWindow,
        requests: keyData.rateLimitRequests
      },
      usage: {
        total: keyData.usage.total,
        daily: keyData.usage.daily,
        monthly: keyData.usage.monthly
      }
    })
  } catch (error) {
    logger.error('Failed to get Azure OpenAI key info:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve API key information',
        type: 'api_error'
      }
    })
  }
})

module.exports = router
