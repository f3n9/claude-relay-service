const crypto = require('crypto')
const { v4: uuidv4 } = require('uuid')
const config = require('../../config/config')
const redis = require('../models/redis')
const logger = require('../utils/logger')
const { DistributedLock } = require('../utils/distributedLock')

// ğŸ”„ Data migration utilities for um-5 â†’ dev compatibility
const DataMigrationUtils = {
  // Transform legacy um-5 data (userId/userUsername) to dev format (owner/ownerType)
  async normalizeApiKeyData(keyData, context = {}) {
    if (!keyData) {
      return keyData
    }

    // ğŸ”’ Handle legacy userId/userUsername â†’ owner/ownerType migration WITH authorization checks
    if (keyData.userId && !keyData.owner) {
      // ğŸ”’ SECURITY: Validate the legacy migration is authorized
      const migrationResult = await this._validateLegacyMigration(keyData, context)

      if (!migrationResult.authorized) {
        logger.security(
          `ğŸš¨ Unauthorized data migration blocked for key ${keyData.id}: ${migrationResult.reason}`
        )

        // è®°å½•å®‰å…¨äº‹ä»¶
        const { securityAudit } = require('../utils/securityAudit')
        securityAudit.logSecurityEvent('DATA_MIGRATION', 'BLOCKED', {
          keyId: keyData.id,
          userId: keyData.userId,
          reason: migrationResult.reason,
          context: context.source || 'unknown',
          timestamp: new Date().toISOString()
        })

        throw new Error('Data migration authorization failed - potential security risk detected')
      }

      // æ‰§è¡Œæˆæƒçš„è¿ç§»
      const originalOwner = keyData.owner
      keyData.owner = keyData.userUsername || keyData.userId
      keyData.ownerType = 'user'

      // ğŸ”’ SECURITY: Log successful migration for audit trail
      logger.security(
        `ğŸ”„ Authorized legacy migration for key ${keyData.id}: userId(${keyData.userId}) â†’ owner(${keyData.owner})`
      )

      const { securityAudit } = require('../utils/securityAudit')
      securityAudit.logSecurityEvent('DATA_MIGRATION', 'SUCCESS', {
        keyId: keyData.id,
        fromUserId: keyData.userId,
        toOwner: keyData.owner,
        previousOwner: originalOwner,
        context: context.source || 'api_validation',
        timestamp: new Date().toISOString()
      })
    }

    // Handle legacy createdBy values
    if (keyData.createdBy === 'user' && !keyData.ownerType) {
      keyData.ownerType = 'user'
    }

    return keyData
  },

  // ğŸ”’ SECURITY: Validate that legacy data migration is authorized
  async _validateLegacyMigration(keyData, context = {}) {
    try {
      // åŸºæœ¬éªŒè¯ï¼šå¿…é¡»æœ‰userIdæ‰èƒ½è¿ç§»
      if (!keyData.userId) {
        return { authorized: false, reason: 'No userId provided for migration' }
      }

      // æ£€æŸ¥userIdæ ¼å¼çš„åˆç†æ€§ï¼ˆåŸºæœ¬åæ»¥ç”¨æ£€æŸ¥ï¼‰
      if (
        typeof keyData.userId !== 'string' ||
        keyData.userId.length > 100 ||
        keyData.userId.length < 1
      ) {
        return { authorized: false, reason: 'Invalid userId format or length' }
      }

      // æ£€æŸ¥æ˜¯å¦åŒ…å«å¯ç–‘å­—ç¬¦ï¼ˆæ³¨å…¥æ£€æŸ¥ï¼‰
      const suspiciousPatterns = [
        /[<>"';&|`$(){}[\]]/, // è„šæœ¬æ³¨å…¥å­—ç¬¦
        /\.\./, // è·¯å¾„éå†
        /\/\//, // URL schemes
        /^(admin|root|system)$/i // ç‰¹æƒç”¨æˆ·å
      ]

      for (const pattern of suspiciousPatterns) {
        if (pattern.test(keyData.userId)) {
          return { authorized: false, reason: 'Suspicious characters in userId' }
        }
      }

      // æ£€æŸ¥userUsernameä¸userIdçš„ä¸€è‡´æ€§ï¼ˆå¦‚æœéƒ½å­˜åœ¨ï¼‰
      if (keyData.userUsername && keyData.userId !== keyData.userUsername) {
        // å…è®¸åˆç†çš„å·®å¼‚ï¼Œä½†è®°å½•ä»¥å¤‡å®¡è®¡
        logger.debug(
          `Username mismatch in migration: userId=${keyData.userId}, userUsername=${keyData.userUsername}`
        )
      }

      // ğŸ”’ é¢‘ç‡é™åˆ¶ï¼šé˜²æ­¢æ‰¹é‡æ»¥ç”¨è¿ç§»
      if (context.source === 'bulk_operation') {
        // å¯¹äºæ‰¹é‡æ“ä½œï¼Œéœ€è¦é¢å¤–çš„éªŒè¯
        logger.security(`ğŸ” Bulk migration detected for userId: ${keyData.userId}`)
      }

      // æ£€æŸ¥æ˜¯å¦å°è¯•é‡å¤è¿ç§»ï¼ˆå¯èƒ½çš„æ”»å‡»æŒ‡æ ‡ï¼‰
      if (
        keyData.owner &&
        keyData.owner !== keyData.userId &&
        keyData.owner !== keyData.userUsername
      ) {
        return {
          authorized: false,
          reason: 'Key already has different owner - potential migration abuse'
        }
      }

      return { authorized: true, reason: 'Migration validation passed' }
    } catch (error) {
      logger.error(`Legacy migration validation error: ${error.message}`)
      return { authorized: false, reason: 'Validation service error - failing securely' }
    }
  },

  // Check if this is legacy um-5 data
  isLegacyData(keyData) {
    return keyData && (keyData.userId || keyData.userUsername) && !keyData.owner
  },

  // Validate legacy user data (backward compatibility)
  async validateLegacyUser(keyData) {
    if (!keyData.userId) {
      return { valid: true }
    }

    try {
      // Try to load userService if it exists (for backward compatibility)
      const userService = require('./userService')
      const user = await userService.getUserById(keyData.userId, false)
      if (!user || !user.isActive) {
        return { valid: false, error: 'User account is disabled' }
      }
      return { valid: true }
    } catch (error) {
      // SECURITY: Fail secure - if userService is unavailable, validation fails
      // This prevents authentication bypass when userService is compromised/unavailable
      logger.security(`ğŸš¨ Legacy user validation failed for ${keyData.userId}: ${error.message}`)
      return {
        valid: false,
        error: 'User validation service unavailable - security policy requires explicit validation'
      }
    }
  }
}

class ApiKeyService {
  constructor() {
    this.prefix = config.security.apiKeyPrefix

    // ğŸ”’ DoS Protection: Rate limiting for failed hash lookups
    this.failedLookupCounter = new Map() // IP/source -> count
    this.circuitBreaker = {
      failureCount: 0,
      lastFailureTime: 0,
      isOpen: false,
      threshold: 10, // Open circuit after 10 failures
      timeout: 60000, // 1 minute timeout
      maxFullScanAttempts: 5 // Maximum full scans per minute
    }

    // ğŸ”’ Race Condition Protection: Distributed lock for hash migration
    this.distributedLock = new DistributedLock(redis.client)
  }

  // ğŸ”’ DoS Protection: Circuit breaker for full key scans
  _isCircuitBreakerOpen() {
    const now = Date.now()

    // Reset circuit breaker if timeout period has passed
    if (
      this.circuitBreaker.isOpen &&
      now - this.circuitBreaker.lastFailureTime > this.circuitBreaker.timeout
    ) {
      this.circuitBreaker.isOpen = false
      this.circuitBreaker.failureCount = 0
      logger.info('ğŸ”„ Circuit breaker reset - allowing hash verification attempts')
    }

    return this.circuitBreaker.isOpen
  }

  // ğŸ”’ DoS Protection: Rate limiting for failed lookups by source
  _shouldAllowFullScan(sourceKey = 'global') {
    const now = Date.now()
    const windowStart = now - 60000 // 1 minute window

    // Clean old entries
    for (const [key, timestamps] of this.failedLookupCounter.entries()) {
      const validTimestamps = timestamps.filter((ts) => ts > windowStart)
      if (validTimestamps.length === 0) {
        this.failedLookupCounter.delete(key)
      } else {
        this.failedLookupCounter.set(key, validTimestamps)
      }
    }

    // Check rate limit for this source
    const attempts = this.failedLookupCounter.get(sourceKey) || []
    const recentAttempts = attempts.filter((ts) => ts > windowStart)

    if (recentAttempts.length >= this.circuitBreaker.maxFullScanAttempts) {
      logger.security(`ğŸš¨ Rate limit exceeded for full API key scan from source: ${sourceKey}`)
      return false
    }

    return true
  }

  // ğŸ”’ DoS Protection: Record failed lookup attempt
  _recordFailedLookup(sourceKey = 'global') {
    const now = Date.now()
    const attempts = this.failedLookupCounter.get(sourceKey) || []
    attempts.push(now)
    this.failedLookupCounter.set(sourceKey, attempts)

    // Update circuit breaker
    this.circuitBreaker.failureCount++
    this.circuitBreaker.lastFailureTime = now

    if (this.circuitBreaker.failureCount >= this.circuitBreaker.threshold) {
      this.circuitBreaker.isOpen = true
      logger.security(
        `ğŸš¨ Circuit breaker opened - too many failed API key lookups (${this.circuitBreaker.failureCount})`
      )
    }
  }

  // ğŸ”‘ ç”Ÿæˆæ–°çš„API Key
  async generateApiKey(options = {}) {
    const {
      name = 'Unnamed Key',
      description = '',
      tokenLimit = config.limits.defaultTokenLimit,
      expiresAt = null,
      claudeAccountId = null,
      claudeConsoleAccountId = null,
      geminiAccountId = null,
      openaiAccountId = null,
      azureOpenaiAccountId = null,
      bedrockAccountId = null, // æ·»åŠ  Bedrock è´¦å·IDæ”¯æŒ
      permissions = 'all', // 'claude', 'gemini', 'openai', 'all'
      isActive = true,
      concurrencyLimit = 0,
      rateLimitWindow = null,
      rateLimitRequests = null,
      enableModelRestriction = false,
      restrictedModels = [],
      enableClientRestriction = false,
      allowedClients = [],
      dailyCostLimit = 0,
      tags = [],
      owner = null,
      ownerType = null
    } = options

    // ç”Ÿæˆç®€å•çš„API Key (64å­—ç¬¦åå…­è¿›åˆ¶)
    const apiKey = `${this.prefix}${this._generateSecretKey()}`
    const keyId = uuidv4()
    const hashedKey = this._hashApiKey(apiKey)

    const keyData = {
      id: keyId,
      name,
      description,
      apiKey: hashedKey,
      tokenLimit: String(tokenLimit ?? 0),
      concurrencyLimit: String(concurrencyLimit ?? 0),
      rateLimitWindow: String(rateLimitWindow ?? 0),
      rateLimitRequests: String(rateLimitRequests ?? 0),
      isActive: String(isActive),
      claudeAccountId: claudeAccountId || '',
      claudeConsoleAccountId: claudeConsoleAccountId || '',
      geminiAccountId: geminiAccountId || '',
      openaiAccountId: openaiAccountId || '',
      azureOpenaiAccountId: azureOpenaiAccountId || '',
      bedrockAccountId: bedrockAccountId || '', // æ·»åŠ  Bedrock è´¦å·ID
      permissions: permissions || 'all',
      enableModelRestriction: String(enableModelRestriction),
      restrictedModels: JSON.stringify(restrictedModels || []),
      enableClientRestriction: String(enableClientRestriction || false),
      allowedClients: JSON.stringify(allowedClients || []),
      dailyCostLimit: String(dailyCostLimit || 0),
      tags: JSON.stringify(tags || []),
      createdAt: new Date().toISOString(),
      lastUsedAt: '',
      expiresAt: expiresAt || '',
      createdBy: 'admin', // å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ç”¨æˆ·ç³»ç»Ÿ
      owner: owner || '',
      ownerType: ownerType || ''
    }

    // ä¿å­˜API Keyæ•°æ®å¹¶å»ºç«‹å“ˆå¸Œæ˜ å°„
    await redis.setApiKey(keyId, keyData, hashedKey)

    logger.success(`ğŸ”‘ Generated new API key: ${name} (${keyId})`)

    return {
      id: keyId,
      apiKey, // åªåœ¨åˆ›å»ºæ—¶è¿”å›å®Œæ•´çš„key
      name: keyData.name,
      description: keyData.description,
      tokenLimit: parseInt(keyData.tokenLimit),
      concurrencyLimit: parseInt(keyData.concurrencyLimit),
      rateLimitWindow: parseInt(keyData.rateLimitWindow || 0),
      rateLimitRequests: parseInt(keyData.rateLimitRequests || 0),
      isActive: keyData.isActive === 'true',
      claudeAccountId: keyData.claudeAccountId,
      claudeConsoleAccountId: keyData.claudeConsoleAccountId,
      geminiAccountId: keyData.geminiAccountId,
      openaiAccountId: keyData.openaiAccountId,
      azureOpenaiAccountId: keyData.azureOpenaiAccountId,
      bedrockAccountId: keyData.bedrockAccountId, // æ·»åŠ  Bedrock è´¦å·ID
      permissions: keyData.permissions,
      enableModelRestriction: keyData.enableModelRestriction === 'true',
      restrictedModels: JSON.parse(keyData.restrictedModels),
      enableClientRestriction: keyData.enableClientRestriction === 'true',
      allowedClients: JSON.parse(keyData.allowedClients || '[]'),
      dailyCostLimit: parseFloat(keyData.dailyCostLimit || 0),
      tags: JSON.parse(keyData.tags || '[]'),
      createdAt: keyData.createdAt,
      expiresAt: keyData.expiresAt,
      createdBy: keyData.createdBy,
      owner: keyData.owner,
      ownerType: keyData.ownerType
    }
  }

  // ğŸ” éªŒè¯API Keyï¼ˆå®‰å…¨å¢å¼ºç‰ˆï¼‰
  async validateApiKey(apiKey) {
    try {
      if (!apiKey || !apiKey.startsWith(this.prefix)) {
        return { valid: false, error: 'Invalid API key format' }
      }

      // ğŸ”’ ä½¿ç”¨å¢å¼ºçš„å“ˆå¸Œæ–¹æ³•è®¡ç®—API Keyå“ˆå¸Œ
      const computedHash = this._hashApiKey(apiKey)

      // ğŸ” é€šè¿‡å“ˆå¸Œå€¼æŸ¥æ‰¾API Keyï¼ˆæ”¯æŒå¤šç‰ˆæœ¬å“ˆå¸Œï¼‰
      let keyData = await redis.findApiKeyByHash(computedHash)

      // å¦‚æœæ–°ç‰ˆæœ¬å“ˆå¸Œæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨æ—§ç‰ˆæœ¬å“ˆå¸ŒæŸ¥æ‰¾
      if (!keyData) {
        const legacyHash = crypto
          .createHash('sha256')
          .update(apiKey + config.security.encryptionKey)
          .digest('hex')

        keyData = await redis.findApiKeyByHash(legacyHash)

        if (keyData) {
          // ğŸ”„ æ‰¾åˆ°äº†ä½¿ç”¨æ—§å“ˆå¸Œçš„API Keyï¼Œè§¦å‘å¼‚æ­¥è¿ç§»
          this._migrateApiKeyHash(keyData.id, apiKey, legacyHash)
          logger.info(`ğŸ”„ Legacy API key hash found, migration triggered: ${keyData.id}`)
        }
      }

      if (!keyData) {
        // ğŸ” æœ€åå°è¯•é€šè¿‡æ‰€æœ‰å·²å­˜å‚¨çš„API Keyè¿›è¡ŒéªŒè¯ï¼ˆå¤„ç†ç‰ˆæœ¬ä¸åŒ¹é…çš„æƒ…å†µï¼‰
        // ğŸ”’ DoS Protection: Check circuit breaker and rate limiting

        const sourceKey = 'api_key_verification' // Could be enhanced with IP-based tracking

        if (this._isCircuitBreakerOpen()) {
          logger.security('ğŸš¨ Circuit breaker is open - blocking full API key scan')
        } else if (!this._shouldAllowFullScan(sourceKey)) {
          logger.security('ğŸš¨ Rate limit exceeded for full API key scan')
          this._recordFailedLookup(sourceKey)
        } else {
          logger.info('ğŸ” Performing full API key scan - last resort verification')

          try {
            const allKeys = await redis.getAllApiKeys()

            // Additional protection: limit the number of keys to check
            const maxKeysToCheck = Math.min(allKeys.length, 1000) // Max 1000 keys per scan
            let checkedCount = 0

            for (const key of allKeys) {
              if (checkedCount >= maxKeysToCheck) {
                logger.security(`ğŸš¨ Full scan limited to ${maxKeysToCheck} keys for DoS protection`)
                break
              }

              if (this._verifyApiKeyHash(apiKey, key.apiKey)) {
                keyData = key
                logger.info(`ğŸ” API key found through hash verification: ${key.id}`)

                // è§¦å‘å“ˆå¸Œè¿ç§»
                this._migrateApiKeyHash(key.id, apiKey, key.apiKey)
                break
              }
              checkedCount++
            }

            if (!keyData) {
              // Record failed lookup for DoS protection
              this._recordFailedLookup(sourceKey)
            }
          } catch (error) {
            logger.error(`Full API key scan failed: ${error.message}`)
            this._recordFailedLookup(sourceKey)
          }
        }
      }

      if (!keyData) {
        // ğŸ”’ è®°å½•å¯ç–‘çš„API KeyéªŒè¯å°è¯•
        const hashedAttempt = crypto.createHash('sha256').update(apiKey).digest('hex').slice(0, 16)
        logger.security(`ğŸš¨ Unknown API key validation attempt: ${hashedAttempt}...`)

        const { securityAudit } = require('../utils/securityAudit')
        securityAudit.logAuthentication('API_KEY_AUTH', 'FAILURE', null, {
          reason: 'api_key_not_found',
          hashedKey: hashedAttempt
        })

        return { valid: false, error: 'API key not found' }
      }

      // ğŸ”„ Apply backward compatibility migration for existing um-5 data
      try {
        keyData = await DataMigrationUtils.normalizeApiKeyData(keyData, {
          source: 'api_validation'
        })
      } catch (migrationError) {
        logger.security(
          `ğŸš¨ Data migration failed for key ${keyData?.id || 'unknown'}: ${migrationError.message}`
        )
        return { valid: false, error: 'Data migration security check failed' }
      }

      // ğŸ” Validate legacy user data if present (backward compatibility)
      if (DataMigrationUtils.isLegacyData(keyData)) {
        const legacyUserValidation = await DataMigrationUtils.validateLegacyUser(keyData)
        if (!legacyUserValidation.valid) {
          return legacyUserValidation
        }
        logger.info(
          `ğŸ”„ Using legacy API key for user ${keyData.owner} (migrated from userId: ${keyData.userId})`
        )
      }

      // æ£€æŸ¥æ˜¯å¦æ¿€æ´»
      if (keyData.isActive !== 'true') {
        logger.security(`ğŸ”’ Disabled API key validation attempt: ${keyData.id}`)
        return { valid: false, error: 'API key is disabled' }
      }

      // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
      if (keyData.expiresAt && new Date() > new Date(keyData.expiresAt)) {
        logger.security(`ğŸ”’ Expired API key validation attempt: ${keyData.id}`)
        return { valid: false, error: 'API key has expired' }
      }

      // è·å–ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¾›è¿”å›æ•°æ®ä½¿ç”¨ï¼‰
      const usage = await redis.getUsageStats(keyData.id)

      // è·å–å½“æ—¥è´¹ç”¨ç»Ÿè®¡
      const dailyCost = await redis.getDailyCost(keyData.id)

      // ğŸ”’ è®°å½•æˆåŠŸçš„API KeyéªŒè¯
      logger.api(`ğŸ”“ API key validated successfully: ${keyData.id}`)

      const { securityAudit } = require('../utils/securityAudit')
      securityAudit.logAuthentication('API_KEY_AUTH', 'SUCCESS', null, {
        keyId: keyData.id,
        keyName: keyData.name
      })

      // è§£æé™åˆ¶æ¨¡å‹æ•°æ®
      let restrictedModels = []
      try {
        restrictedModels = keyData.restrictedModels ? JSON.parse(keyData.restrictedModels) : []
      } catch (e) {
        restrictedModels = []
      }

      // è§£æå…è®¸çš„å®¢æˆ·ç«¯
      let allowedClients = []
      try {
        allowedClients = keyData.allowedClients ? JSON.parse(keyData.allowedClients) : []
      } catch (e) {
        allowedClients = []
      }

      // è§£ææ ‡ç­¾
      let tags = []
      try {
        tags = keyData.tags ? JSON.parse(keyData.tags) : []
      } catch (e) {
        tags = []
      }

      return {
        valid: true,
        keyData: {
          id: keyData.id,
          name: keyData.name,
          description: keyData.description,
          createdAt: keyData.createdAt,
          expiresAt: keyData.expiresAt,
          claudeAccountId: keyData.claudeAccountId,
          claudeConsoleAccountId: keyData.claudeConsoleAccountId,
          geminiAccountId: keyData.geminiAccountId,
          openaiAccountId: keyData.openaiAccountId,
          azureOpenaiAccountId: keyData.azureOpenaiAccountId,
          bedrockAccountId: keyData.bedrockAccountId, // æ·»åŠ  Bedrock è´¦å·ID
          permissions: keyData.permissions || 'all',
          tokenLimit: parseInt(keyData.tokenLimit),
          concurrencyLimit: parseInt(keyData.concurrencyLimit || 0),
          rateLimitWindow: parseInt(keyData.rateLimitWindow || 0),
          rateLimitRequests: parseInt(keyData.rateLimitRequests || 0),
          enableModelRestriction: keyData.enableModelRestriction === 'true',
          restrictedModels,
          enableClientRestriction: keyData.enableClientRestriction === 'true',
          allowedClients,
          dailyCostLimit: parseFloat(keyData.dailyCostLimit || 0),
          dailyCost: dailyCost || 0,
          tags,
          usage
        }
      }
    } catch (error) {
      logger.error('âŒ API key validation error:', error)

      const { securityAudit } = require('../utils/securityAudit')
      securityAudit.logAuthentication('API_KEY_AUTH', 'ERROR', null, {
        error: error.message,
        errorType: 'validation_error'
      })

      return { valid: false, error: 'Internal validation error' }
    }
  }

  // ğŸ“‹ è·å–æ‰€æœ‰API Keys
  async getAllApiKeys() {
    try {
      const apiKeys = await redis.getAllApiKeys()
      const client = redis.getClientSafe()

      // ä¸ºæ¯ä¸ªkeyæ·»åŠ ä½¿ç”¨ç»Ÿè®¡å’Œå½“å‰å¹¶å‘æ•°
      for (const key of apiKeys) {
        // ğŸ”„ Apply backward compatibility migration to each key
        try {
          await DataMigrationUtils.normalizeApiKeyData(key, { source: 'bulk_listing' })
        } catch (migrationError) {
          logger.security(
            `ğŸš¨ Skipping key ${key?.id || 'unknown'} due to migration error: ${migrationError.message}`
          )
          continue // Skip this key if migration fails
        }
        key.usage = await redis.getUsageStats(key.id)
        key.tokenLimit = parseInt(key.tokenLimit)
        key.concurrencyLimit = parseInt(key.concurrencyLimit || 0)
        key.rateLimitWindow = parseInt(key.rateLimitWindow || 0)
        key.rateLimitRequests = parseInt(key.rateLimitRequests || 0)
        key.currentConcurrency = await redis.getConcurrency(key.id)
        key.isActive = key.isActive === 'true'
        key.enableModelRestriction = key.enableModelRestriction === 'true'
        key.enableClientRestriction = key.enableClientRestriction === 'true'
        key.permissions = key.permissions || 'all' // å…¼å®¹æ—§æ•°æ®
        key.dailyCostLimit = parseFloat(key.dailyCostLimit || 0)
        key.dailyCost = (await redis.getDailyCost(key.id)) || 0

        // è·å–å½“å‰æ—¶é—´çª—å£çš„è¯·æ±‚æ¬¡æ•°å’ŒTokenä½¿ç”¨é‡
        if (key.rateLimitWindow > 0) {
          const requestCountKey = `rate_limit:requests:${key.id}`
          const tokenCountKey = `rate_limit:tokens:${key.id}`
          const windowStartKey = `rate_limit:window_start:${key.id}`

          key.currentWindowRequests = parseInt((await client.get(requestCountKey)) || '0')
          key.currentWindowTokens = parseInt((await client.get(tokenCountKey)) || '0')

          // è·å–çª—å£å¼€å§‹æ—¶é—´å’Œè®¡ç®—å‰©ä½™æ—¶é—´
          const windowStart = await client.get(windowStartKey)
          if (windowStart) {
            const now = Date.now()
            const windowStartTime = parseInt(windowStart)
            const windowDuration = key.rateLimitWindow * 60 * 1000 // è½¬æ¢ä¸ºæ¯«ç§’
            const windowEndTime = windowStartTime + windowDuration

            // å¦‚æœçª—å£è¿˜æœ‰æ•ˆ
            if (now < windowEndTime) {
              key.windowStartTime = windowStartTime
              key.windowEndTime = windowEndTime
              key.windowRemainingSeconds = Math.max(0, Math.floor((windowEndTime - now) / 1000))
            } else {
              // çª—å£å·²è¿‡æœŸï¼Œä¸‹æ¬¡è¯·æ±‚ä¼šé‡ç½®
              key.windowStartTime = null
              key.windowEndTime = null
              key.windowRemainingSeconds = 0
              // é‡ç½®è®¡æ•°ä¸º0ï¼Œå› ä¸ºçª—å£å·²è¿‡æœŸ
              key.currentWindowRequests = 0
              key.currentWindowTokens = 0
            }
          } else {
            // çª—å£è¿˜æœªå¼€å§‹ï¼ˆæ²¡æœ‰ä»»ä½•è¯·æ±‚ï¼‰
            key.windowStartTime = null
            key.windowEndTime = null
            key.windowRemainingSeconds = null
          }
        } else {
          key.currentWindowRequests = 0
          key.currentWindowTokens = 0
          key.windowStartTime = null
          key.windowEndTime = null
          key.windowRemainingSeconds = null
        }

        try {
          key.restrictedModels = key.restrictedModels ? JSON.parse(key.restrictedModels) : []
        } catch (e) {
          key.restrictedModels = []
        }
        try {
          key.allowedClients = key.allowedClients ? JSON.parse(key.allowedClients) : []
        } catch (e) {
          key.allowedClients = []
        }
        try {
          key.tags = key.tags ? JSON.parse(key.tags) : []
        } catch (e) {
          key.tags = []
        }
        delete key.apiKey // ä¸è¿”å›å“ˆå¸Œåçš„key
      }

      return apiKeys
    } catch (error) {
      logger.error('âŒ Failed to get API keys:', error)
      throw error
    }
  }

  // ğŸ“ æ›´æ–°API Key
  async updateApiKey(keyId, updates) {
    try {
      logger.debug(`ğŸ”§ Updating API key ${keyId} with:`, updates)

      const keyData = await redis.getApiKey(keyId)
      if (!keyData || Object.keys(keyData).length === 0) {
        logger.error(`âŒ API key not found: ${keyId}`)
        throw new Error('API key not found')
      }

      logger.debug(`ğŸ“‹ Current API key data:`, {
        id: keyData.id,
        name: keyData.name,
        owner: keyData.owner,
        ownerType: keyData.ownerType
      })

      // å…è®¸æ›´æ–°çš„å­—æ®µ
      const allowedUpdates = [
        'name',
        'description',
        'tokenLimit',
        'concurrencyLimit',
        'rateLimitWindow',
        'rateLimitRequests',
        'isActive',
        'claudeAccountId',
        'claudeConsoleAccountId',
        'geminiAccountId',
        'openaiAccountId',
        'azureOpenaiAccountId',
        'bedrockAccountId', // æ·»åŠ  Bedrock è´¦å·ID
        'permissions',
        'expiresAt',
        'enableModelRestriction',
        'restrictedModels',
        'enableClientRestriction',
        'allowedClients',
        'dailyCostLimit',
        'tags'
      ]
      const updatedData = { ...keyData }

      for (const [field, value] of Object.entries(updates)) {
        if (allowedUpdates.includes(field)) {
          if (field === 'restrictedModels' || field === 'allowedClients' || field === 'tags') {
            // ç‰¹æ®Šå¤„ç†æ•°ç»„å­—æ®µ
            updatedData[field] = JSON.stringify(value || [])
          } else if (field === 'enableModelRestriction' || field === 'enableClientRestriction') {
            // å¸ƒå°”å€¼è½¬å­—ç¬¦ä¸²
            updatedData[field] = String(value)
          } else {
            updatedData[field] = (value !== null && value !== undefined ? value : '').toString()
          }
        }
      }

      updatedData.updatedAt = new Date().toISOString()

      // æ›´æ–°æ—¶ä¸éœ€è¦é‡æ–°å»ºç«‹å“ˆå¸Œæ˜ å°„ï¼Œå› ä¸ºAPI Keyæœ¬èº«æ²¡æœ‰å˜åŒ–
      await redis.setApiKey(keyId, updatedData)

      logger.success(`ğŸ“ Updated API key: ${keyId}`, {
        updatedFields: Object.keys(updates),
        newName: updatedData.name
      })

      return { success: true }
    } catch (error) {
      logger.error('âŒ Failed to update API key:', error)
      throw error
    }
  }

  // ğŸ—‘ï¸ åˆ é™¤API Key
  async deleteApiKey(keyId) {
    try {
      const result = await redis.deleteApiKey(keyId)

      if (result === 0) {
        throw new Error('API key not found')
      }

      logger.success(`ğŸ—‘ï¸ Deleted API key: ${keyId}`)

      return { success: true }
    } catch (error) {
      logger.error('âŒ Failed to delete API key:', error)
      throw error
    }
  }

  // ğŸ“Š è®°å½•ä½¿ç”¨æƒ…å†µï¼ˆæ”¯æŒç¼“å­˜tokenå’Œè´¦æˆ·çº§åˆ«ç»Ÿè®¡ï¼‰
  async recordUsage(
    keyId,
    inputTokens = 0,
    outputTokens = 0,
    cacheCreateTokens = 0,
    cacheReadTokens = 0,
    model = 'unknown',
    accountId = null
  ) {
    try {
      const totalTokens = inputTokens + outputTokens + cacheCreateTokens + cacheReadTokens

      // è®¡ç®—è´¹ç”¨
      const CostCalculator = require('../utils/costCalculator')
      const costInfo = CostCalculator.calculateCost(
        {
          input_tokens: inputTokens,
          output_tokens: outputTokens,
          cache_creation_input_tokens: cacheCreateTokens,
          cache_read_input_tokens: cacheReadTokens
        },
        model
      )

      // è®°å½•API Keyçº§åˆ«çš„ä½¿ç”¨ç»Ÿè®¡
      await redis.incrementTokenUsage(
        keyId,
        totalTokens,
        inputTokens,
        outputTokens,
        cacheCreateTokens,
        cacheReadTokens,
        model
      )

      // è®°å½•è´¹ç”¨ç»Ÿè®¡
      if (costInfo.costs.total > 0) {
        await redis.incrementDailyCost(keyId, costInfo.costs.total)
        logger.database(
          `ğŸ’° Recorded cost for ${keyId}: $${costInfo.costs.total.toFixed(6)}, model: ${model}`
        )
      } else {
        logger.debug(`ğŸ’° No cost recorded for ${keyId} - zero cost for model: ${model}`)
      }

      // è·å–API Keyæ•°æ®ä»¥ç¡®å®šå…³è”çš„è´¦æˆ·
      const keyData = await redis.getApiKey(keyId)
      if (keyData && Object.keys(keyData).length > 0) {
        // æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
        keyData.lastUsedAt = new Date().toISOString()
        await redis.setApiKey(keyId, keyData)

        // è®°å½•è´¦æˆ·çº§åˆ«çš„ä½¿ç”¨ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡å®é™…å¤„ç†è¯·æ±‚çš„è´¦æˆ·ï¼‰
        if (accountId) {
          await redis.incrementAccountUsage(
            accountId,
            totalTokens,
            inputTokens,
            outputTokens,
            cacheCreateTokens,
            cacheReadTokens,
            model
          )
          logger.database(
            `ğŸ“Š Recorded account usage: ${accountId} - ${totalTokens} tokens (API Key: ${keyId})`
          )
        } else {
          logger.debug(
            'âš ï¸ No accountId provided for usage recording, skipping account-level statistics'
          )
        }
      }

      const logParts = [`Model: ${model}`, `Input: ${inputTokens}`, `Output: ${outputTokens}`]
      if (cacheCreateTokens > 0) {
        logParts.push(`Cache Create: ${cacheCreateTokens}`)
      }
      if (cacheReadTokens > 0) {
        logParts.push(`Cache Read: ${cacheReadTokens}`)
      }
      logParts.push(`Total: ${totalTokens} tokens`)

      logger.database(`ğŸ“Š Recorded usage: ${keyId} - ${logParts.join(', ')}`)
    } catch (error) {
      logger.error('âŒ Failed to record usage:', error)
    }
  }

  // ğŸ“Š è®°å½•ä½¿ç”¨æƒ…å†µï¼ˆæ–°ç‰ˆæœ¬ï¼Œæ”¯æŒè¯¦ç»†çš„ç¼“å­˜ç±»å‹ï¼‰
  async recordUsageWithDetails(keyId, usageObject, model = 'unknown', accountId = null) {
    try {
      // æå– token æ•°é‡
      const inputTokens = usageObject.input_tokens || 0
      const outputTokens = usageObject.output_tokens || 0
      const cacheCreateTokens = usageObject.cache_creation_input_tokens || 0
      const cacheReadTokens = usageObject.cache_read_input_tokens || 0

      const totalTokens = inputTokens + outputTokens + cacheCreateTokens + cacheReadTokens

      // è®¡ç®—è´¹ç”¨ï¼ˆæ”¯æŒè¯¦ç»†çš„ç¼“å­˜ç±»å‹ï¼‰- æ·»åŠ é”™è¯¯å¤„ç†
      let costInfo = { totalCost: 0, ephemeral5mCost: 0, ephemeral1hCost: 0 }
      try {
        const pricingService = require('./pricingService')
        // ç¡®ä¿ pricingService å·²åˆå§‹åŒ–
        if (!pricingService.pricingData) {
          logger.warn('âš ï¸ PricingService not initialized, initializing now...')
          await pricingService.initialize()
        }
        costInfo = pricingService.calculateCost(usageObject, model)
      } catch (pricingError) {
        logger.error('âŒ Failed to calculate cost:', pricingError)
        // ç»§ç»­æ‰§è¡Œï¼Œä¸è¦å› ä¸ºè´¹ç”¨è®¡ç®—å¤±è´¥è€Œè·³è¿‡ç»Ÿè®¡è®°å½•
      }

      // æå–è¯¦ç»†çš„ç¼“å­˜åˆ›å»ºæ•°æ®
      let ephemeral5mTokens = 0
      let ephemeral1hTokens = 0

      if (usageObject.cache_creation && typeof usageObject.cache_creation === 'object') {
        ephemeral5mTokens = usageObject.cache_creation.ephemeral_5m_input_tokens || 0
        ephemeral1hTokens = usageObject.cache_creation.ephemeral_1h_input_tokens || 0
      }

      // è®°å½•API Keyçº§åˆ«çš„ä½¿ç”¨ç»Ÿè®¡ - è¿™ä¸ªå¿…é¡»æ‰§è¡Œ
      await redis.incrementTokenUsage(
        keyId,
        totalTokens,
        inputTokens,
        outputTokens,
        cacheCreateTokens,
        cacheReadTokens,
        model,
        ephemeral5mTokens, // ä¼ é€’5åˆ†é’Ÿç¼“å­˜ tokens
        ephemeral1hTokens // ä¼ é€’1å°æ—¶ç¼“å­˜ tokens
      )

      // è®°å½•è´¹ç”¨ç»Ÿè®¡
      if (costInfo.totalCost > 0) {
        await redis.incrementDailyCost(keyId, costInfo.totalCost)
        logger.database(
          `ğŸ’° Recorded cost for ${keyId}: $${costInfo.totalCost.toFixed(6)}, model: ${model}`
        )

        // è®°å½•è¯¦ç»†çš„ç¼“å­˜è´¹ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if (costInfo.ephemeral5mCost > 0 || costInfo.ephemeral1hCost > 0) {
          logger.database(
            `ğŸ’° Cache costs - 5m: $${costInfo.ephemeral5mCost.toFixed(6)}, 1h: $${costInfo.ephemeral1hCost.toFixed(6)}`
          )
        }
      } else {
        logger.debug(`ğŸ’° No cost recorded for ${keyId} - zero cost for model: ${model}`)
      }

      // è·å–API Keyæ•°æ®ä»¥ç¡®å®šå…³è”çš„è´¦æˆ·
      const keyData = await redis.getApiKey(keyId)
      if (keyData && Object.keys(keyData).length > 0) {
        // æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
        keyData.lastUsedAt = new Date().toISOString()
        await redis.setApiKey(keyId, keyData)

        // è®°å½•è´¦æˆ·çº§åˆ«çš„ä½¿ç”¨ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡å®é™…å¤„ç†è¯·æ±‚çš„è´¦æˆ·ï¼‰
        if (accountId) {
          await redis.incrementAccountUsage(
            accountId,
            totalTokens,
            inputTokens,
            outputTokens,
            cacheCreateTokens,
            cacheReadTokens,
            model
          )
          logger.database(
            `ğŸ“Š Recorded account usage: ${accountId} - ${totalTokens} tokens (API Key: ${keyId})`
          )
        } else {
          logger.debug(
            'âš ï¸ No accountId provided for usage recording, skipping account-level statistics'
          )
        }
      }

      const logParts = [`Model: ${model}`, `Input: ${inputTokens}`, `Output: ${outputTokens}`]
      if (cacheCreateTokens > 0) {
        logParts.push(`Cache Create: ${cacheCreateTokens}`)

        // å¦‚æœæœ‰è¯¦ç»†çš„ç¼“å­˜åˆ›å»ºæ•°æ®ï¼Œä¹Ÿè®°å½•å®ƒä»¬
        if (usageObject.cache_creation) {
          const { ephemeral_5m_input_tokens, ephemeral_1h_input_tokens } =
            usageObject.cache_creation
          if (ephemeral_5m_input_tokens > 0) {
            logParts.push(`5m: ${ephemeral_5m_input_tokens}`)
          }
          if (ephemeral_1h_input_tokens > 0) {
            logParts.push(`1h: ${ephemeral_1h_input_tokens}`)
          }
        }
      }
      if (cacheReadTokens > 0) {
        logParts.push(`Cache Read: ${cacheReadTokens}`)
      }
      logParts.push(`Total: ${totalTokens} tokens`)

      logger.database(`ğŸ“Š Recorded usage: ${keyId} - ${logParts.join(', ')}`)
    } catch (error) {
      logger.error('âŒ Failed to record usage:', error)
    }
  }

  // ğŸ” ç”Ÿæˆå¯†é’¥
  _generateSecretKey() {
    return crypto.randomBytes(32).toString('hex')
  }

  // ğŸ”’ å“ˆå¸ŒAPI Keyï¼ˆå¢å¼ºç‰ˆ - æŠ—ç¢°æ’æ”»å‡»ï¼‰
  _hashApiKey(apiKey) {
    try {
      // ğŸ”„ ä½¿ç”¨å¢å¼ºçš„å“ˆå¸Œæ–¹æ³•ï¼ˆv2ï¼‰
      return this._hashApiKeyV2(apiKey)
    } catch (error) {
      logger.error('âŒ V2 API Key hashing error:', error)
      // å›é€€åˆ°æ—§ç‰ˆæœ¬å“ˆå¸Œï¼ˆå‘åå…¼å®¹ï¼‰
      return this._hashApiKeyV1(apiKey)
    }
  }

  // ğŸ”’ V2 API Key å“ˆå¸Œæ–¹æ³•ï¼ˆä½¿ç”¨PBKDF2å¢å¼ºå®‰å…¨æ€§ï¼‰
  _hashApiKeyV2(apiKey) {
    try {
      // ç”ŸæˆåŸºäºAPI Keyçš„ç¡®å®šæ€§ç›ï¼ˆç¡®ä¿ç›¸åŒçš„API Keyæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„å“ˆå¸Œï¼‰
      const salt = crypto
        .createHash('sha256')
        .update(`${apiKey}v2_salt_${config.security.encryptionKey}`)
        .digest()

      // ä½¿ç”¨PBKDF2è¿›è¡Œå¯†é’¥æ´¾ç”Ÿï¼ˆ10,000æ¬¡è¿­ä»£ï¼‰
      const hash = crypto.pbkdf2Sync(apiKey, salt, 10000, 64, 'sha256')

      // æ ¼å¼: v2:salt:hash
      const saltHex = salt.toString('hex')
      const hashHex = hash.toString('hex')
      const combinedHash = `v2:${saltHex}:${hashHex}`

      logger.debug('ğŸ”’ API Key hashed using V2 method (PBKDF2)')
      return combinedHash
    } catch (error) {
      logger.error('âŒ V2 API Key hashing error:', error)
      throw error
    }
  }

  // ğŸ”’ V1 API Key å“ˆå¸Œæ–¹æ³•ï¼ˆåŸå§‹æ–¹æ³• - å‘åå…¼å®¹ï¼‰
  _hashApiKeyV1(apiKey) {
    try {
      const hash = crypto
        .createHash('sha256')
        .update(apiKey + config.security.encryptionKey)
        .digest('hex')

      // ä¸ºæ—§æ ¼å¼æ·»åŠ v1å‰ç¼€ä»¥ä¾¿è¯†åˆ«
      return `v1:${hash}`
    } catch (error) {
      logger.error('âŒ V1 API Key hashing error:', error)
      throw error
    }
  }

  // ğŸ” éªŒè¯API Keyå“ˆå¸Œï¼ˆæ”¯æŒå¤šç‰ˆæœ¬ï¼‰
  _verifyApiKeyHash(apiKey, storedHash) {
    try {
      // ğŸ” æ£€æµ‹å“ˆå¸Œç‰ˆæœ¬å¹¶ä½¿ç”¨å¯¹åº”çš„éªŒè¯æ–¹æ³•
      if (storedHash.startsWith('v2:')) {
        return this._verifyApiKeyHashV2(apiKey, storedHash)
      } else if (storedHash.startsWith('v1:')) {
        return this._verifyApiKeyHashV1(apiKey, storedHash)
      } else {
        // Legacy format (no version prefix)
        return this._verifyApiKeyHashLegacy(apiKey, storedHash)
      }
    } catch (error) {
      logger.error('âŒ API Key hash verification error:', error)
      return false
    }
  }

  // ğŸ” V2å“ˆå¸ŒéªŒè¯
  _verifyApiKeyHashV2(apiKey, storedHash) {
    try {
      // é‡æ–°è®¡ç®—å“ˆå¸Œå¹¶æ¯”è¾ƒ
      const computedHash = this._hashApiKeyV2(apiKey)

      // ä½¿ç”¨æ—¶é—´å¸¸æ•°æ¯”è¾ƒé˜²æ­¢æ—¶åºæ”»å‡»
      return this._secureCompare(computedHash, storedHash)
    } catch (error) {
      logger.error('âŒ V2 hash verification error:', error)
      return false
    }
  }

  // ğŸ” V1å“ˆå¸ŒéªŒè¯
  _verifyApiKeyHashV1(apiKey, storedHash) {
    try {
      // ç§»é™¤v1å‰ç¼€
      const hashWithoutPrefix = storedHash.substring(3)
      const computedHash = crypto
        .createHash('sha256')
        .update(apiKey + config.security.encryptionKey)
        .digest('hex')

      return this._secureCompare(computedHash, hashWithoutPrefix)
    } catch (error) {
      logger.error('âŒ V1 hash verification error:', error)
      return false
    }
  }

  // ğŸ” Legacyå“ˆå¸ŒéªŒè¯ï¼ˆåŸå§‹æ ¼å¼ï¼‰
  _verifyApiKeyHashLegacy(apiKey, storedHash) {
    try {
      const computedHash = crypto
        .createHash('sha256')
        .update(apiKey + config.security.encryptionKey)
        .digest('hex')

      return this._secureCompare(computedHash, storedHash)
    } catch (error) {
      logger.error('âŒ Legacy hash verification error:', error)
      return false
    }
  }

  // ğŸ”’ æ—¶é—´å¸¸æ•°æ¯”è¾ƒï¼ˆé˜²æ­¢æ—¶åºæ”»å‡»ï¼‰
  _secureCompare(a, b) {
    if (a.length !== b.length) {
      return false
    }

    let result = 0
    for (let i = 0; i < a.length; i++) {
      result |= a.charCodeAt(i) ^ b.charCodeAt(i)
    }
    return result === 0
  }

  // ğŸ”„ å¼‚æ­¥å“ˆå¸Œè¿ç§»ï¼ˆåœ¨éªŒè¯æ—¶å‡çº§æ—§å“ˆå¸Œï¼‰ - ä½¿ç”¨åˆ†å¸ƒå¼é”é˜²æ­¢ç«æ€æ¡ä»¶
  async _migrateApiKeyHash(keyId, apiKey, currentHash) {
    // ğŸ”’ ä½¿ç”¨åˆ†å¸ƒå¼é”é˜²æ­¢å¹¶å‘è¿ç§»å¯¼è‡´çš„ç«æ€æ¡ä»¶
    const lockResource = `hash_migration:${keyId}`
    const lockTTL = 30000 // 30ç§’é”è¶…æ—¶
    const maxRetries = 3
    const retryDelay = 1000

    try {
      // åªè¿ç§»æ—§ç‰ˆæœ¬çš„å“ˆå¸Œ
      if (currentHash.startsWith('v2:')) {
        return // å·²ç»æ˜¯æœ€æ–°ç‰ˆæœ¬
      }

      // å¼‚æ­¥æ‰§è¡Œè¿ç§»ä»¥é¿å…é˜»å¡å½“å‰è¯·æ±‚
      setImmediate(async () => {
        let lockAcquired = false

        try {
          // ğŸ”’ è·å–åˆ†å¸ƒå¼é”
          lockAcquired = await this.distributedLock.acquire(
            [lockResource],
            lockTTL,
            maxRetries,
            retryDelay,
            1 // æ­£å¸¸ä¼˜å…ˆçº§
          )

          if (!lockAcquired) {
            logger.debug(`â³ Hash migration for ${keyId} skipped - already in progress`)
            return
          }

          // å†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»ï¼ˆå¯èƒ½åœ¨ç­‰å¾…é”æœŸé—´å·²è¢«å…¶ä»–è¿›ç¨‹è¿ç§»ï¼‰
          const currentKeyData = await redis.getApiKey(keyId)
          if (!currentKeyData || Object.keys(currentKeyData).length === 0) {
            logger.debug(`âš ï¸  API key ${keyId} not found during migration`)
            return
          }

          if (currentKeyData.apiKey.startsWith('v2:')) {
            logger.debug(`âœ… API key ${keyId} already migrated to V2`)
            return
          }

          // ç”Ÿæˆæ–°çš„V2å“ˆå¸Œ
          const newHash = this._hashApiKeyV2(apiKey)

          // åŸå­æ€§åœ°æ›´æ–°æ•°æ®
          currentKeyData.apiKey = newHash
          await redis.setApiKey(keyId, currentKeyData, newHash)

          // æ¸…ç†æ—§çš„å“ˆå¸Œæ˜ å°„
          await redis.client.del(`api_key_hash:${currentHash}`)

          logger.info(`ğŸ”„ Successfully migrated API key hash to V2: ${keyId}`)
        } catch (migrationError) {
          logger.error(`âŒ Hash migration failed for ${keyId}:`, migrationError)

          // è®°å½•å®‰å…¨äº‹ä»¶
          const { securityAudit } = require('../utils/securityAudit')
          securityAudit.logSecurityEvent('HASH_MIGRATION', 'ERROR', {
            keyId,
            error: migrationError.message,
            timestamp: new Date().toISOString()
          })
        } finally {
          // ğŸ”“ é‡Šæ”¾åˆ†å¸ƒå¼é”
          if (lockAcquired) {
            try {
              await this.distributedLock.release([lockResource])
            } catch (releaseError) {
              logger.error(`âŒ Failed to release migration lock for ${keyId}:`, releaseError)
            }
          }
        }
      })
    } catch (error) {
      logger.debug('Hash migration preparation failed:', error.message)
    }
  }

  // ğŸ•µï¸ ç¢°æ’æ£€æµ‹ï¼ˆæ£€æµ‹æ½œåœ¨çš„å“ˆå¸Œç¢°æ’ï¼‰
  async _detectHashCollisions() {
    try {
      const allKeys = await redis.getAllApiKeys()
      const hashCounts = new Map()
      const collisions = []

      // ç»Ÿè®¡æ¯ä¸ªå“ˆå¸Œå‡ºç°çš„æ¬¡æ•°
      for (const key of allKeys) {
        const hash = key.apiKey
        if (hashCounts.has(hash)) {
          hashCounts.set(hash, hashCounts.get(hash) + 1)
        } else {
          hashCounts.set(hash, 1)
        }
      }

      // æŸ¥æ‰¾ç¢°æ’ï¼ˆåŒä¸€ä¸ªå“ˆå¸Œå¯¹åº”å¤šä¸ªå¯†é’¥ï¼‰
      for (const [hash, count] of hashCounts.entries()) {
        if (count > 1) {
          const keysWithSameHash = allKeys
            .filter((key) => key.apiKey === hash)
            .map((key) => ({ id: key.id, name: key.name }))

          collisions.push({
            hash: `${hash.substring(0, 16)}...`, // åªæ˜¾ç¤ºéƒ¨åˆ†å“ˆå¸Œç”¨äºæ—¥å¿—
            count,
            keys: keysWithSameHash
          })
        }
      }

      if (collisions.length > 0) {
        logger.error(`ğŸš¨ Hash collisions detected: ${collisions.length} cases`, collisions)

        // å‘é€å®‰å…¨å‘Šè­¦
        const { securityAudit } = require('../utils/securityAudit')
        securityAudit.logSecurityViolation(
          'HASH_COLLISION',
          'API Key hash collision detected',
          'LOGGED',
          null,
          { collisionCount: collisions.length, details: collisions }
        )
      }

      return collisions
    } catch (error) {
      logger.error('âŒ Hash collision detection error:', error)
      return []
    }
  }

  // ğŸ“ˆ è·å–ä½¿ç”¨ç»Ÿè®¡
  async getUsageStats(keyId) {
    return await redis.getUsageStats(keyId)
  }

  // ğŸ“Š è·å–è´¦æˆ·ä½¿ç”¨ç»Ÿè®¡
  async getAccountUsageStats(accountId) {
    return await redis.getAccountUsageStats(accountId)
  }

  // ğŸ“ˆ è·å–æ‰€æœ‰è´¦æˆ·ä½¿ç”¨ç»Ÿè®¡
  async getAllAccountsUsageStats() {
    return await redis.getAllAccountsUsageStats()
  }

  // ğŸ§¹ æ¸…ç†è¿‡æœŸçš„API Keys
  async cleanupExpiredKeys() {
    try {
      const apiKeys = await redis.getAllApiKeys()
      const now = new Date()
      let cleanedCount = 0

      for (const key of apiKeys) {
        // æ£€æŸ¥æ˜¯å¦å·²è¿‡æœŸä¸”ä»å¤„äºæ¿€æ´»çŠ¶æ€
        if (key.expiresAt && new Date(key.expiresAt) < now && key.isActive === 'true') {
          // å°†è¿‡æœŸçš„ API Key æ ‡è®°ä¸ºç¦ç”¨çŠ¶æ€ï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ é™¤
          await this.updateApiKey(key.id, { isActive: false })
          logger.info(`ğŸ”’ API Key ${key.id} (${key.name}) has expired and been disabled`)
          cleanedCount++
        }
      }

      if (cleanedCount > 0) {
        logger.success(`ğŸ§¹ Disabled ${cleanedCount} expired API keys`)
      }

      return cleanedCount
    } catch (error) {
      logger.error('âŒ Failed to cleanup expired keys:', error)
      return 0
    }
  }

  // ğŸ”„ Legacy Methods for Backward Compatibility (um-5 â†’ dev transition)

  // ğŸ‘¤ Get API Keys by user (legacy compatibility method)
  async getUserApiKeys(userId, includeDeleted = false) {
    try {
      const allKeys = await this.getAllApiKeys()
      // Support both legacy userId and new owner formats
      let userKeys = allKeys.filter(
        (key) => key.userId === userId || (key.owner === userId && key.ownerType === 'user')
      )

      if (!includeDeleted) {
        userKeys = userKeys.filter((key) => !key.isDeleted || key.isDeleted !== 'true')
      }

      // Transform to include both old and new formats for compatibility
      return userKeys.map((key) => ({
        ...key,
        // Preserve legacy fields for existing integrations
        userId: key.userId || (key.ownerType === 'user' ? key.owner : ''),
        userUsername: key.userUsername || (key.ownerType === 'user' ? key.owner : ''),
        owner: key.owner,
        ownerType: key.ownerType
      }))
    } catch (error) {
      logger.error('âŒ Failed to get user API keys:', error)
      throw error
    }
  }

  // ğŸ” Get API Key by ID with user permission check (legacy compatibility)
  async getApiKeyById(keyId, userId = null) {
    try {
      const keyData = await redis.getApiKey(keyId)
      if (!keyData) {
        return null
      }

      // Apply migration with security checks
      try {
        await DataMigrationUtils.normalizeApiKeyData(keyData, { source: 'individual_lookup' })
      } catch (migrationError) {
        logger.security(`ğŸš¨ Data migration failed for key ${keyId}: ${migrationError.message}`)
        return null // Fail securely - don't return potentially compromised data
      }

      // Check permissions (support both legacy and new formats)
      if (
        userId &&
        keyData.userId !== userId &&
        !(keyData.owner === userId && keyData.ownerType === 'user')
      ) {
        return null
      }

      const usage = await redis.getUsageStats(keyData.id)
      const costStats = await redis.getCostStats(keyData.id)

      return {
        ...keyData,
        usage,
        costStats,
        // Include both formats for compatibility
        userId: keyData.userId || (keyData.ownerType === 'user' ? keyData.owner : ''),
        userUsername: keyData.userUsername || (keyData.ownerType === 'user' ? keyData.owner : ''),
        owner: keyData.owner,
        ownerType: keyData.ownerType
      }
    } catch (error) {
      logger.error('âŒ Failed to get API key by ID:', error)
      throw error
    }
  }

  // ğŸš« Disable user API Keys (legacy compatibility method)
  async disableUserApiKeys(userId) {
    try {
      const userKeys = await this.getUserApiKeys(userId)
      let disabledCount = 0

      for (const key of userKeys) {
        if (key.isActive === 'true' || key.isActive === true) {
          await this.updateApiKey(key.id, { isActive: false })
          disabledCount++
        }
      }

      logger.info(`ğŸš« Disabled ${disabledCount} API keys for user: ${userId}`)
      return { count: disabledCount }
    } catch (error) {
      logger.error('âŒ Failed to disable user API keys:', error)
      throw error
    }
  }
}

// å¯¼å‡ºå®ä¾‹å’Œå•ç‹¬çš„æ–¹æ³•
const apiKeyService = new ApiKeyService()

// ä¸ºäº†æ–¹ä¾¿å…¶ä»–æœåŠ¡è°ƒç”¨ï¼Œå¯¼å‡º recordUsage æ–¹æ³•
apiKeyService.recordUsageMetrics = apiKeyService.recordUsage.bind(apiKeyService)

module.exports = apiKeyService
